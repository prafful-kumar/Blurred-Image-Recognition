{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deblurring U_NET(Keras).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPuO6dGabki0KFSTG/XpLyF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prafful-kumar/Unet-Deblur/blob/main/Deblurring_U_NET(Keras).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnUrghuqeoTH",
        "outputId": "84e466b1-b7d8-4698-81cf-f848ae8aeed3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FN6xPehWerck"
      },
      "source": [
        "import os, os.path, errno\n",
        "import argparse\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.image import array_to_img\n",
        "#from models import modelsClass\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tensorflow as tf\n",
        "from datetime import datetime \n",
        "import cv2\n",
        "from keras import backend, optimizers"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDboBGkmfCqZ"
      },
      "source": [
        "Defining Architecture of the deblur model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mgyjb0kyfN7U"
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, concatenate, Conv2D, Conv2DTranspose, MaxPooling2D, ZeroPadding2D\n",
        "from keras import backend as keras\n",
        "\n",
        "class Deblur(object):\n",
        "\n",
        "    def __init__(self, img_rows = 270, img_cols = 480):\n",
        "\n",
        "        self.img_rows = img_rows\n",
        "        self.img_cols = img_cols\n",
        "\n",
        "    def addPadding(self, layer, level): #height, width, level):\n",
        "    \n",
        "        w1, h1 = self.img_cols, self.img_rows\n",
        "        w2, h2 = int(w1/2), int(h1/2)\n",
        "        w3, h3 = int(w2/2), int(h2/2)\n",
        "        w4, h4 = int(w3/2), int(h3/2)\n",
        "        h = [h1,h2,h3,h4]\n",
        "        w = [w1,w2,w3,w4]\n",
        "        \n",
        "        # Target width and height\n",
        "        tw = w[level-1]\n",
        "        th = h[level-1]\n",
        "        \n",
        "        # Source width and height\n",
        "        lsize = keras.int_shape(layer)\n",
        "        sh = lsize[1]\n",
        "        sw = lsize[2]\n",
        "\n",
        "        pw = (0, tw - sw)\n",
        "        ph = (0, th - sh)\n",
        "\n",
        "        layer = ZeroPadding2D(padding=(ph,pw),data_format=\"channels_last\")(layer)\n",
        "    \n",
        "        return layer\n",
        "        \n",
        "    def Blind_deblur(self):\n",
        "\n",
        "        input_blurred = Input((self.img_rows, self.img_cols,3))\n",
        "        filter_in = 64\n",
        "        filter_out = 64\n",
        "        # conv = layers.Dropout(dropout)(conv)\n",
        "        conv64 = Conv2D(filter_in, (3, 3), activation='relu', padding='same')(input_blurred)\n",
        "        conv64 = Conv2D(filter_in, (3, 3), activation='relu', padding='same')(conv64)\n",
        "        conv64 = BatchNormalization(axis=3)(conv64)\n",
        "        pool1 = MaxPooling2D(pool_size=(2, 2))(conv64)\n",
        "\n",
        "        conv128 = Conv2D(filter_in *2, (3, 3), activation='relu', padding='same')(pool1)\n",
        "        conv128 = Conv2D(filter_in *2, (3, 3), activation='relu', padding='same')(conv128)\n",
        "        conv128 = BatchNormalization(axis=3)(conv128)\n",
        "        pool2 = MaxPooling2D(pool_size=(2, 2))(conv128)\n",
        "\n",
        "        conv256 = Conv2D(filter_in *4, (3, 3), activation='relu', padding='same')(pool2)\n",
        "        conv256 = Conv2D(filter_in *4, (3, 3), activation='relu', padding='same')(conv256)\n",
        "        conv256 = BatchNormalization(axis=3)(conv256)\n",
        "        pool3 = MaxPooling2D(pool_size=(2, 2))(conv256)\n",
        "\n",
        "        conv512 = Conv2D(filter_in*8, (3, 3), activation='relu', padding='same')(pool3)\n",
        "        conv512 = Conv2D(filter_in*8, (3, 3), activation='relu', padding='same')(conv512)\n",
        "        conv512 = BatchNormalization(axis=3)(conv512)\n",
        "        pool4 = MaxPooling2D(pool_size=(2, 2))(conv512)\n",
        "        # bottleneck layer\n",
        "        conv1024 = Conv2D(filter_in*16, (3, 3), activation='relu', padding='same')(pool4)\n",
        "        conv1024 = Conv2D(filter_in*16, (3, 3), activation='relu', padding='same')(conv1024)\n",
        "\n",
        "        up6 = Conv2DTranspose(filter_in*8, (2, 2), strides=(2, 2), padding='same')(conv1024)\n",
        "        up6 = self.addPadding(up6,level=4)\n",
        "        up6 = concatenate([up6,conv512], axis=3)\n",
        "        up_conv512 = Conv2D(filter_in*8, (3, 3), activation='relu', padding='same')(up6)\n",
        "        up_conv512 = Conv2D(filter_in*8, (3, 3), activation='relu', padding='same')(up_conv512)\n",
        "        up_conv512 = BatchNormalization(axis=3)(up_conv512)\n",
        "\n",
        "        up7 = Conv2DTranspose(filter_in*4, (2, 2), strides=(2, 2), padding='same')(up_conv512)\n",
        "        up7 = self.addPadding(up7,level=3)\n",
        "        up7 = concatenate([up7,conv256], axis=3)\n",
        "        up_conv256 = Conv2D(filter_in*4, (3, 3), activation='relu', padding='same')(up7)\n",
        "        up_conv256 = Conv2D(filter_in*4, (3, 3), activation='relu', padding='same')(up_conv256)\n",
        "        up_conv256 = BatchNormalization(axis=3)(up_conv256)\n",
        "\n",
        "        up8 = Conv2DTranspose(filter_in*2, (2, 2), strides=(2, 2), padding='same')(up_conv256)\n",
        "        up8 = self.addPadding(up8,level=2)\n",
        "        up8 = concatenate([up8,conv128], axis=3)\n",
        "        up_conv128 = Conv2D(filter_in*2, (3, 3), activation='relu', padding='same')(up8)\n",
        "        up_conv128 = Conv2D(filter_in*2, (3, 3), activation='relu', padding='same')(up_conv128)\n",
        "        up_conv128 = BatchNormalization(axis=3)(up_conv128)\n",
        "\n",
        "        up9 = Conv2DTranspose(filter_in, (2, 2), strides=(2, 2), padding='same')(up_conv128)\n",
        "        up9 = self.addPadding(up9,level=1)\n",
        "        up9 = concatenate([up9,conv64], axis=3)\n",
        "        up_conv64 = Conv2D(filter_in, (3, 3), activation='relu', padding='same')(up9)\n",
        "        up_conv64 = Conv2D(filter_in, (3, 3), activation='relu', padding='same')(up_conv64)\n",
        "        up_conv64 = BatchNormalization(axis=3)(up_conv64)\n",
        "\n",
        "        conv10 = Conv2D(filter_out, (1, 1), activation='tanh')(up_conv64)\n",
        "        \n",
        "        model = Model(inputs=input_blurred, outputs=conv10)\n",
        "        \n",
        "        return model\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atH5bEnLxvK6",
        "outputId": "80608335-077e-486f-cae6-762f36e386e8"
      },
      "source": [
        "!cp -av \"/content/drive/MyDrive/Train/Validation.tar.gz\" \"/content/sample_data\"\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'/content/drive/MyDrive/Train/Validation.tar.gz' -> '/content/sample_data/Validation.tar.gz'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40NjRw2xx3Ur"
      },
      "source": [
        "!tar --gunzip --extract --verbose --file=\"/content/sample_data/Validation.tar.gz\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHc6UxVMfN97"
      },
      "source": [
        "image_directory = \"/content/Validation/sharp/sharp\"\n",
        "blur_directory = \"/content/Validation/blurred/blurred\"\n",
        "\n",
        "width = 480\n",
        "height = 270\n",
        "image_dataset = [] \n",
        "blur_dataset = []  \n",
        "\n",
        "images = os.listdir(image_directory)\n",
        "for i, image_name in enumerate(images):    #Remember enumerate method adds a counter and returns the enumerate object\n",
        "    if (image_name.split('.')[1] == 'png'):\n",
        "        #print(image_directory+image_name)\n",
        "        image = cv2.imread(image_directory+\"/\"+image_name)\n",
        "        image = Image.fromarray(image)\n",
        "        \n",
        "        image = image.resize((width,height))\n",
        "        image_dataset.append((1./255)*np.array(image))\n",
        "\n",
        "\n",
        "blurs = os.listdir(blur_directory)\n",
        "for i, image_name in enumerate(blurs):\n",
        "    if (image_name.split('.')[1] == 'png'):\n",
        "        image = cv2.imread(blur_directory+\"/\"+ image_name)\n",
        "        image = Image.fromarray(image)\n",
        "        \n",
        "        image = image.resize((width,height))\n",
        "        blur_dataset.append((1./255)*np.array(image))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkIxSioPfD4e"
      },
      "source": [
        "outpath = \"output (demo)\"\n",
        "try:\n",
        "    os.makedirs(outpath)\n",
        "except OSError as e:\n",
        "    if e.errno != errno.EEXIST:\n",
        "        raise"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdV-EdKbigT6"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(image_dataset, blur_dataset, test_size = 0.20, random_state = 42)\n",
        "\n",
        "#Sanity check, view few mages\n",
        "import random\n",
        "import numpy as np\n",
        "image_number = random.randint(0, len(X_train))\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(121)\n",
        "plt.imshow(np.reshape(X_train[image_number], (height,width, 3)))\n",
        "plt.subplot(122)\n",
        "plt.imshow(np.reshape(y_train[image_number], (height,width,3)))\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-bSFiTzW1Sd"
      },
      "source": [
        "Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oysi8T6gzDDJ"
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def jacard_coef(y_true, y_pred):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (intersection + 1.0) / (K.sum(y_true_f) + K.sum(y_pred_f) - intersection + 1.0)\n",
        "\n",
        "\n",
        "def jacard_coef_loss(y_true, y_pred):\n",
        "    return -jacard_coef(y_true, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5k4rjbRDW6BF"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppxWHEAHfIXW"
      },
      "source": [
        "blurred_path = inpath\n",
        "blurred_names = os.listdir(blurred_path)\n",
        "\n",
        "width = 480\n",
        "height = 270\n",
        "models = Deblur(height,width)\n",
        "model = models.Blind_deblur()\n",
        "\n",
        "adam = optimizers.Adam(lr=0.0000125, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "model.compile(optimizer = adam, loss = 'mean_squared_error',metrics=['accuracy', jacard_coef])\n",
        "\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "\n",
        "start3 = datetime.now() \n",
        "model_history = model.fit(X_train, y_train, \n",
        "                    verbose=1,\n",
        "                    batch_size = 32,\n",
        "                    validation_data=(X_test, y_test ), \n",
        "                    shuffle=False,\n",
        "                    epochs=50)\n",
        "stop3 = datetime.now()\n",
        "\n",
        "#Execution time of the model \n",
        "execution_time_model = stop3-start3\n",
        "print(\"Model's execution time is: \", execution_time_model)\n",
        "\n",
        "model.save('Unet_50epochs_mse.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGst3YXyizmo"
      },
      "source": [
        "Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7xrCQADixZK"
      },
      "source": [
        "for fname in blurred_names:\n",
        "        \n",
        "    print(\"Deblurring '%s' with DeepBlind\" %(fname))\n",
        "             \n",
        "    blurred_img = Image.open(blurred_path + fname)\n",
        "    blurred_img = blurred_img.resize((width,height),resample = 3)\n",
        "    blurred_np = (1./255)*np.array(blurred_img)\n",
        "    \n",
        "            \n",
        "    \n",
        "            \n",
        "    x = np.reshape(blurred_np,[1,height,width,3])\n",
        "    prediction = model.predict(x, batch_size=1,verbose=0,steps=None)\n",
        "    prediction = prediction[0,:,:,:]\n",
        "            \n",
        "    deblurred_img = array_to_img(prediction)\n",
        "    deblurred_img.save(outpath+\"/%s\"%(fname))\n",
        "            \n",
        "print(\"DONE!\")\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}